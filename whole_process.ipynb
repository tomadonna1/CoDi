{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88142d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from typing import Tuple, List, Dict, Any, Optional\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15a3777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetProcessor:\n",
    "    \"\"\"Complete dataset processor for CoDi with automatic fixes and validation\"\"\"\n",
    "    \n",
    "    def __init__(self, categorical_threshold: int = 20, numeric_categorical_threshold: float = 0.05):\n",
    "        self.categorical_threshold = categorical_threshold\n",
    "        self.numeric_categorical_threshold = numeric_categorical_threshold\n",
    "    \n",
    "    def auto_detect_column_types(self, df: pd.DataFrame) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"Automatically detect continuous and categorical columns with improved logic\"\"\"\n",
    "        continuous_cols = []\n",
    "        categorical_cols = []\n",
    "        \n",
    "        print(\"Analyzing column types...\")\n",
    "        for col in df.columns:\n",
    "            col_data = df[col].dropna()  # Remove NaN for analysis\n",
    "            unique_count = col_data.nunique()\n",
    "            total_count = len(col_data)\n",
    "            unique_ratio = unique_count / total_count if total_count > 0 else 0\n",
    "            \n",
    "            # Check if column contains only integers (potential categorical)\n",
    "            is_integer_like = False\n",
    "            if pd.api.types.is_numeric_dtype(col_data):\n",
    "                is_integer_like = col_data.apply(lambda x: float(x).is_integer()).all()\n",
    "            \n",
    "            # Enhanced decision logic\n",
    "            if pd.api.types.is_numeric_dtype(col_data):\n",
    "                # Special case: floating point values that are actually discrete\n",
    "                if is_integer_like and (unique_count <= self.categorical_threshold or unique_ratio < self.numeric_categorical_threshold):\n",
    "                    categorical_cols.append(col)\n",
    "                    print(f\"'{col}': Numeric categorical ({unique_count} unique, ratio: {unique_ratio:.3f})\")\n",
    "                # Special case: many decimal values suggest continuous\n",
    "                elif not is_integer_like and unique_count > self.categorical_threshold:\n",
    "                    continuous_cols.append(col)\n",
    "                    print(f\"'{col}': Continuous decimal ({unique_count} unique)\")\n",
    "                # Default numeric logic\n",
    "                elif unique_count <= self.categorical_threshold or unique_ratio < self.numeric_categorical_threshold:\n",
    "                    categorical_cols.append(col)\n",
    "                    print(f\"'{col}': Numeric categorical ({unique_count} unique, ratio: {unique_ratio:.3f})\")\n",
    "                else:\n",
    "                    continuous_cols.append(col)\n",
    "                    print(f\"'{col}': Continuous ({unique_count} unique)\")\n",
    "            else:\n",
    "                # Non-numeric -> categorical\n",
    "                categorical_cols.append(col)\n",
    "                print(f\"'{col}': Text categorical ({unique_count} unique)\")\n",
    "        \n",
    "        return continuous_cols, categorical_cols\n",
    "    \n",
    "    def preprocess_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"Enhanced preprocessing with better missing value handling\"\"\"\n",
    "        df_processed = df.copy()\n",
    "        categorical_mappings = {}\n",
    "        \n",
    "        print(\"\\nPreprocessing data...\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        for col in df_processed.columns:\n",
    "            if df_processed[col].isnull().any():\n",
    "                null_count = df_processed[col].isnull().sum()\n",
    "                if pd.api.types.is_numeric_dtype(df_processed[col]):\n",
    "                    df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "                    print(f\"Filled {null_count} missing values in '{col}' with median\")\n",
    "                else:\n",
    "                    mode_val = df_processed[col].mode()\n",
    "                    if len(mode_val) > 0:\n",
    "                        df_processed[col].fillna(mode_val[0], inplace=True)\n",
    "                    else:\n",
    "                        df_processed[col].fillna('unknown', inplace=True)\n",
    "                    print(f\"Filled {null_count} missing values in '{col}' with mode/unknown\")\n",
    "        \n",
    "        # Encode categorical variables with proper indexing\n",
    "        for col in df_processed.columns:\n",
    "            if not pd.api.types.is_numeric_dtype(df_processed[col]):\n",
    "                unique_vals = sorted(df_processed[col].unique())\n",
    "                mapping = {val: idx for idx, val in enumerate(unique_vals)}\n",
    "                categorical_mappings[col] = {\n",
    "                    'mapping': mapping,\n",
    "                    'reverse_mapping': {idx: val for val, idx in mapping.items()}\n",
    "                }\n",
    "                df_processed[col] = df_processed[col].map(mapping)\n",
    "                print(f\"Encoded '{col}': {len(unique_vals)} categories -> [0, {len(unique_vals)-1}]\")\n",
    "        \n",
    "        return df_processed, categorical_mappings\n",
    "    \n",
    "    def validate_and_fix_categorical_data(self, data: np.ndarray, columns: List[Dict]) -> Tuple[np.ndarray, List[Dict]]:\n",
    "        \"\"\"Validate and fix categorical columns to ensure proper 0-based indexing\"\"\"\n",
    "        print(\"\\nValidating and fixing categorical data...\")\n",
    "        fixed_data = data.copy()\n",
    "        fixed_columns = [col.copy() for col in columns]\n",
    "        \n",
    "        for i, col in enumerate(fixed_columns):\n",
    "            if col['type'] == 'categorical':\n",
    "                col_data = fixed_data[:, i].astype(int)\n",
    "                unique_vals = sorted(np.unique(col_data))\n",
    "                \n",
    "                # Check if values are properly 0-based\n",
    "                expected_range = list(range(len(unique_vals)))\n",
    "                if unique_vals != expected_range:\n",
    "                    print(f\"Fixing '{col['name']}': {unique_vals} -> {expected_range}\")\n",
    "                    \n",
    "                    # Create mapping to fix indexing\n",
    "                    mapping = {old_val: new_val for new_val, old_val in enumerate(unique_vals)}\n",
    "                    \n",
    "                    # Apply mapping\n",
    "                    for old_val, new_val in mapping.items():\n",
    "                        fixed_data[fixed_data[:, i] == old_val, i] = new_val\n",
    "                    \n",
    "                    # Update column metadata\n",
    "                    col['size'] = len(unique_vals)\n",
    "                    col['i2s'] = [str(val) for val in unique_vals]\n",
    "                else:\n",
    "                    print(f\"'{col['name']}': Already properly indexed [0, {len(unique_vals)-1}]\")\n",
    "        \n",
    "        return fixed_data, fixed_columns\n",
    "    \n",
    "    def create_codi_format(self, dataset_name: str, train_data: np.ndarray, test_data: np.ndarray, \n",
    "                          column_names: List[str], con_idx: List[int], dis_idx: List[int], \n",
    "                          categorical_mappings: Dict) -> Dict:\n",
    "        \"\"\"Create CoDi-compatible format with validation\"\"\"\n",
    "        \n",
    "        # Validate and fix categorical data\n",
    "        all_data = np.vstack([train_data, test_data])\n",
    "        \n",
    "        # Create initial columns structure\n",
    "        columns = []\n",
    "        for i, col_name in enumerate(column_names):\n",
    "            if i in con_idx:\n",
    "                col_data = all_data[:, i]\n",
    "                columns.append({\n",
    "                    \"name\": col_name,\n",
    "                    \"type\": \"continuous\",\n",
    "                    \"min\": float(np.min(col_data)),\n",
    "                    \"max\": float(np.max(col_data))\n",
    "                })\n",
    "            else:\n",
    "                col_data = all_data[:, i].astype(int)\n",
    "                unique_vals = sorted(np.unique(col_data))\n",
    "                \n",
    "                # Create i2s mapping\n",
    "                if col_name in categorical_mappings:\n",
    "                    reverse_mapping = categorical_mappings[col_name]['reverse_mapping']\n",
    "                    i2s = [str(reverse_mapping.get(idx, str(idx))) for idx in unique_vals]\n",
    "                else:\n",
    "                    i2s = [str(val) for val in unique_vals]\n",
    "                \n",
    "                columns.append({\n",
    "                    \"name\": col_name,\n",
    "                    \"type\": \"categorical\",\n",
    "                    \"size\": len(unique_vals),\n",
    "                    \"i2s\": i2s\n",
    "                })\n",
    "        \n",
    "        # Fix categorical data indexing\n",
    "        fixed_train, fixed_columns = self.validate_and_fix_categorical_data(train_data, columns)\n",
    "        fixed_test, _ = self.validate_and_fix_categorical_data(test_data, columns)\n",
    "        \n",
    "        # Determine problem type\n",
    "        last_col_idx = len(column_names) - 1\n",
    "        if last_col_idx in dis_idx:\n",
    "            last_col_name = column_names[last_col_idx]\n",
    "            if last_col_name in categorical_mappings:\n",
    "                num_classes = len(categorical_mappings[last_col_name]['mapping'])\n",
    "            else:\n",
    "                num_classes = len(np.unique(all_data[:, last_col_idx]))\n",
    "            \n",
    "            problem_type = \"binary_classification\" if num_classes == 2 else \"multiclass_classification\"\n",
    "        else:\n",
    "            problem_type = \"regression\"\n",
    "        \n",
    "        # Save fixed data\n",
    "        os.makedirs('tabular_datasets', exist_ok=True)\n",
    "        np.savez(f'tabular_datasets/{dataset_name}.npz', train=fixed_train, test=fixed_test)\n",
    "        \n",
    "        # Create metadata\n",
    "        codi_meta = {\n",
    "            \"columns\": fixed_columns,\n",
    "            \"problem_type\": problem_type\n",
    "        }\n",
    "        \n",
    "        with open(f'tabular_datasets/{dataset_name}.json', 'w') as f:\n",
    "            json.dump(codi_meta, f, indent=2)\n",
    "        \n",
    "        return codi_meta\n",
    "    \n",
    "    def process_dataset(self, csv_path: str, dataset_name: str, \n",
    "                       force_continuous: Optional[List[str]] = None,\n",
    "                       force_categorical: Optional[List[str]] = None,\n",
    "                       test_split: float = 0.2,\n",
    "                       create_backup: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Complete dataset processing pipeline\"\"\"\n",
    "        \n",
    "        force_continuous = force_continuous or []\n",
    "        force_categorical = force_categorical or []\n",
    "        \n",
    "        print(f\"Processing dataset: {csv_path} -> {dataset_name}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Create backup if requested\n",
    "        if create_backup and os.path.exists(f'tabular_datasets/{dataset_name}.npz'):\n",
    "            print(\"Creating backup of existing dataset...\")\n",
    "            shutil.copy(f'tabular_datasets/{dataset_name}.npz', f'tabular_datasets/{dataset_name}_backup.npz')\n",
    "            if os.path.exists(f'tabular_datasets/{dataset_name}.json'):\n",
    "                shutil.copy(f'tabular_datasets/{dataset_name}.json', f'tabular_datasets/{dataset_name}_backup.json')\n",
    "        \n",
    "        # Load and analyze data\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Original shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Auto-detect column types\n",
    "        continuous_cols, categorical_cols = self.auto_detect_column_types(df)\n",
    "        \n",
    "        # Apply manual overrides\n",
    "        if force_continuous or force_categorical:\n",
    "            print(f\"\\nApplying manual overrides...\")\n",
    "            for col in force_continuous:\n",
    "                if col in categorical_cols:\n",
    "                    categorical_cols.remove(col)\n",
    "                if col not in continuous_cols:\n",
    "                    continuous_cols.append(col)\n",
    "                print(f\"Forced '{col}' to continuous\")\n",
    "            \n",
    "            for col in force_categorical:\n",
    "                if col in continuous_cols:\n",
    "                    continuous_cols.remove(col)\n",
    "                if col not in categorical_cols:\n",
    "                    categorical_cols.append(col)\n",
    "                print(f\"Forced '{col}' to categorical\")\n",
    "        \n",
    "        print(f\"\\nFinal column assignment:\")\n",
    "        print(f\"Continuous ({len(continuous_cols)}): {continuous_cols}\")\n",
    "        print(f\"Categorical ({len(categorical_cols)}): {categorical_cols}\")\n",
    "        \n",
    "        # Preprocess data\n",
    "        df_processed, categorical_mappings = self.preprocess_data(df)\n",
    "        \n",
    "        # Get indices\n",
    "        con_idx = [df_processed.columns.get_loc(col) for col in continuous_cols]\n",
    "        dis_idx = [df_processed.columns.get_loc(col) for col in categorical_cols]\n",
    "        \n",
    "        # Split data\n",
    "        data = df_processed.values.astype(np.float32)\n",
    "        n_samples, n_features = data.shape\n",
    "        n_test = int(n_samples * test_split)\n",
    "        \n",
    "        # Shuffle and split\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        test_data = data[indices[:n_test]]\n",
    "        train_data = data[indices[n_test:]]\n",
    "        \n",
    "        print(f\"\\nData split:\")\n",
    "        print(f\"Training samples: {len(train_data)}\")\n",
    "        print(f\"Test samples: {len(test_data)}\")\n",
    "        \n",
    "        # Create CoDi format with validation and fixes\n",
    "        codi_meta = self.create_codi_format(\n",
    "            dataset_name, train_data, test_data, \n",
    "            df_processed.columns.tolist(), con_idx, dis_idx, categorical_mappings\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nDataset '{dataset_name}' processed successfully!\")\n",
    "        print(f\"Problem type: {codi_meta['problem_type']}\")\n",
    "        print(f\"Files created:\")\n",
    "        print(f\"  - tabular_datasets/{dataset_name}.npz\")\n",
    "        print(f\"  - tabular_datasets/{dataset_name}.json\")\n",
    "        \n",
    "        return {\n",
    "            'dataset_name': dataset_name,\n",
    "            'shape': (n_samples, n_features),\n",
    "            'problem_type': codi_meta['problem_type'],\n",
    "            'continuous_columns': continuous_cols,\n",
    "            'categorical_columns': categorical_cols,\n",
    "            'train_samples': len(train_data),\n",
    "            'test_samples': len(test_data)\n",
    "        }\n",
    "    \n",
    "    def validate_dataset(self, dataset_name: str) -> bool:\n",
    "        \"\"\"Validate that a dataset is properly formatted for CoDi\"\"\"\n",
    "        try:\n",
    "            data = np.load(f'tabular_datasets/{dataset_name}.npz')\n",
    "            with open(f'tabular_datasets/{dataset_name}.json', 'r') as f:\n",
    "                meta = json.load(f)\n",
    "            \n",
    "            train_data = data['train']\n",
    "            print(f\"Validating dataset: {dataset_name}\")\n",
    "            print(\"=\"*40)\n",
    "            \n",
    "            issues_found = False\n",
    "            \n",
    "            for i, col in enumerate(meta['columns']):\n",
    "                if col['type'] == 'categorical':\n",
    "                    col_data = train_data[:, i].astype(int)\n",
    "                    unique_vals = np.unique(col_data)\n",
    "                    min_val, max_val = np.min(col_data), np.max(col_data)\n",
    "                    \n",
    "                    if min_val < 0 or max_val >= col['size']:\n",
    "                        print(f\"{col['name']}: values [{min_val}, {max_val}] outside expected [0, {col['size']-1}]\")\n",
    "                        issues_found = True\n",
    "                    else:\n",
    "                        print(f\"{col['name']}: properly indexed [0, {col['size']-1}]\")\n",
    "                else:\n",
    "                    print(f\"{col['name']}: continuous column OK\")\n",
    "            \n",
    "            if not issues_found:\n",
    "                print(f\"\\nDataset '{dataset_name}' is valid for CoDi training!\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"\\nDataset '{dataset_name}' has validation issues.\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Validation failed: {e}\")\n",
    "            return False\n",
    "\n",
    "# Usage functions\n",
    "def quick_process(csv_path: str, dataset_name: str, **kwargs) -> Dict[str, Any]:\n",
    "    \"\"\"Quick processing with default settings\"\"\"\n",
    "    processor = DatasetProcessor()\n",
    "    return processor.process_dataset(csv_path, dataset_name, **kwargs)\n",
    "\n",
    "def process_with_overrides(csv_path: str, dataset_name: str, \n",
    "                          continuous_cols: List[str] = None,\n",
    "                          categorical_cols: List[str] = None, **kwargs) -> Dict[str, Any]:\n",
    "    \"\"\"Process with manual column type specification\"\"\"\n",
    "    processor = DatasetProcessor()\n",
    "    return processor.process_dataset(\n",
    "        csv_path, dataset_name, \n",
    "        force_continuous=continuous_cols,\n",
    "        force_categorical=categorical_cols,\n",
    "        **kwargs\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b6c1e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id               0\n",
       "SepalLengthCm    0\n",
       "SepalWidthCm     0\n",
       "PetalLengthCm    0\n",
       "PetalWidthCm     0\n",
       "Species          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_original = pd.read_csv(\"raw_data/iris.csv\")\n",
    "iris_original.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe87ce03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: raw_data/Buddy.csv -> Buddy\n",
      "============================================================\n",
      "Original shape: (18832, 8)\n",
      "Columns: ['condition', 'color_type', 'length_m', 'height_cm', 'x1', 'x2', 'breed_category', 'pet_category']\n",
      "Analyzing column types...\n",
      "'condition': Numeric categorical (3 unique, ratio: 0.000)\n",
      "'color_type': Text categorical (56 unique)\n",
      "'length_m': Continuous decimal (101 unique)\n",
      "'height_cm': Continuous decimal (4425 unique)\n",
      "'x1': Numeric categorical (20 unique, ratio: 0.001)\n",
      "'x2': Numeric categorical (10 unique, ratio: 0.001)\n",
      "'breed_category': Numeric categorical (3 unique, ratio: 0.000)\n",
      "'pet_category': Numeric categorical (4 unique, ratio: 0.000)\n",
      "\n",
      "Final column assignment:\n",
      "Continuous (2): ['length_m', 'height_cm']\n",
      "Categorical (6): ['condition', 'color_type', 'x1', 'x2', 'breed_category', 'pet_category']\n",
      "\n",
      "Preprocessing data...\n",
      "Filled 1477 missing values in 'condition' with median\n",
      "Encoded 'color_type': 56 categories -> [0, 55]\n",
      "\n",
      "Data split:\n",
      "Training samples: 15066\n",
      "Test samples: 3766\n",
      "\n",
      "Validating and fixing categorical data...\n",
      "'condition': Already properly indexed [0, 2]\n",
      "'color_type': Already properly indexed [0, 55]\n",
      "'x1': Already properly indexed [0, 19]\n",
      "'x2': Already properly indexed [0, 9]\n",
      "'breed_category': Already properly indexed [0, 2]\n",
      "Fixing 'pet_category': [0, 1, 2, 4] -> [0, 1, 2, 3]\n",
      "\n",
      "Validating and fixing categorical data...\n",
      "'condition': Already properly indexed [0, 2]\n",
      "Fixing 'color_type': [1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]\n",
      "Fixing 'x1': [0, 1, 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 18] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Fixing 'x2': [1, 2, 3, 4, 5, 6, 7, 8, 9] -> [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "'breed_category': Already properly indexed [0, 2]\n",
      "Fixing 'pet_category': [0, 1, 2, 4] -> [0, 1, 2, 3]\n",
      "\n",
      "Dataset 'Buddy' processed successfully!\n",
      "Problem type: multiclass_classification\n",
      "Files created:\n",
      "  - tabular_datasets/Buddy.npz\n",
      "  - tabular_datasets/Buddy.json\n",
      "Validating dataset: Buddy\n",
      "========================================\n",
      "condition: properly indexed [0, 2]\n",
      "color_type: properly indexed [0, 55]\n",
      "length_m: continuous column OK\n",
      "height_cm: continuous column OK\n",
      "x1: properly indexed [0, 19]\n",
      "x2: properly indexed [0, 9]\n",
      "breed_category: properly indexed [0, 2]\n",
      "pet_category: properly indexed [0, 3]\n",
      "\n",
      "Dataset 'Buddy' is valid for CoDi training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "processor = DatasetProcessor(categorical_threshold=15, numeric_categorical_threshold=0.05)\n",
    "\n",
    "# Process a dataset\n",
    "result = processor.process_dataset(\n",
    "    csv_path='raw_data/Buddy.csv',\n",
    "    dataset_name='Buddy',\n",
    "    test_split=0.2,\n",
    "    create_backup=True\n",
    ")\n",
    "\n",
    "# Validate the result\n",
    "processor.validate_dataset('Buddy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532ad66",
   "metadata": {},
   "source": [
    "python main.py --data iris --logdir CoDi_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc8cfa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: I0902 21:54:56.000986 140639798581056 main.py:93] Co-evolving Conditional Diffusion models\n",
      "I0902 21:54:59.709763 140639798581056 co_evolving_condition.py:69] Continuous model params: 450628\n",
      "I0902 21:54:59.710226 140639798581056 co_evolving_condition.py:70] Discrete model params: 690288\n",
      "I0902 21:54:59.710445 140639798581056 co_evolving_condition.py:76] Total steps: 300\n",
      "I0902 21:54:59.710609 140639798581056 co_evolving_condition.py:77] Sample steps: 30000\n",
      "I0902 21:54:59.710780 140639798581056 co_evolving_condition.py:78] Continuous: 15066, 2\n",
      "I0902 21:54:59.710942 140639798581056 co_evolving_condition.py:79] Discrete: 15066, 96\n",
      "I0902 21:55:02.147573 140639798581056 co_evolving_condition.py:126] Epoch :0, diffusion continuous loss: 1.014, discrete loss: 0.275\n",
      "I0902 21:55:02.148159 140639798581056 co_evolving_condition.py:127] Epoch :0, CL continuous loss: 1.000, discrete loss: 1.000\n",
      "I0902 21:55:02.148564 140639798581056 co_evolving_condition.py:128] Epoch :0, Total continuous loss: 1.214, discrete loss: 0.475\n",
      "I0902 21:55:03.558981 140639798581056 co_evolving_condition.py:126] Epoch :1, diffusion continuous loss: 1.080, discrete loss: 0.265\n",
      "I0902 21:55:03.559535 140639798581056 co_evolving_condition.py:127] Epoch :1, CL continuous loss: 0.999, discrete loss: 1.000\n",
      "I0902 21:55:03.559851 140639798581056 co_evolving_condition.py:128] Epoch :1, Total continuous loss: 1.280, discrete loss: 0.465\n",
      "I0902 21:55:04.736574 140639798581056 co_evolving_condition.py:126] Epoch :2, diffusion continuous loss: 1.027, discrete loss: 0.254\n",
      "I0902 21:55:04.737183 140639798581056 co_evolving_condition.py:127] Epoch :2, CL continuous loss: 1.002, discrete loss: 1.000\n",
      "I0902 21:55:04.737567 140639798581056 co_evolving_condition.py:128] Epoch :2, Total continuous loss: 1.227, discrete loss: 0.454\n",
      "I0902 21:55:05.975796 140639798581056 co_evolving_condition.py:126] Epoch :3, diffusion continuous loss: 0.958, discrete loss: 0.250\n",
      "I0902 21:55:05.976322 140639798581056 co_evolving_condition.py:127] Epoch :3, CL continuous loss: 0.999, discrete loss: 1.000\n",
      "I0902 21:55:05.976605 140639798581056 co_evolving_condition.py:128] Epoch :3, Total continuous loss: 1.158, discrete loss: 0.450\n",
      "I0902 21:55:07.252336 140639798581056 co_evolving_condition.py:126] Epoch :4, diffusion continuous loss: 1.044, discrete loss: 0.254\n",
      "I0902 21:55:07.252842 140639798581056 co_evolving_condition.py:127] Epoch :4, CL continuous loss: 1.000, discrete loss: 1.000\n",
      "I0902 21:55:07.253099 140639798581056 co_evolving_condition.py:128] Epoch :4, Total continuous loss: 1.244, discrete loss: 0.454\n",
      "I0902 21:55:08.512339 140639798581056 co_evolving_condition.py:126] Epoch :5, diffusion continuous loss: 1.017, discrete loss: 0.243\n",
      "I0902 21:55:08.512843 140639798581056 co_evolving_condition.py:127] Epoch :5, CL continuous loss: 1.000, discrete loss: 1.000\n",
      "I0902 21:55:08.513101 140639798581056 co_evolving_condition.py:128] Epoch :5, Total continuous loss: 1.217, discrete loss: 0.443\n",
      "I0902 21:55:09.963757 140639798581056 co_evolving_condition.py:126] Epoch :6, diffusion continuous loss: 0.984, discrete loss: 0.232\n",
      "I0902 21:55:09.978535 140639798581056 co_evolving_condition.py:127] Epoch :6, CL continuous loss: 1.000, discrete loss: 1.000\n",
      "I0902 21:55:09.979047 140639798581056 co_evolving_condition.py:128] Epoch :6, Total continuous loss: 1.184, discrete loss: 0.432\n",
      "I0902 21:55:11.220695 140639798581056 co_evolving_condition.py:126] Epoch :7, diffusion continuous loss: 1.072, discrete loss: 0.227\n",
      "I0902 21:55:11.221171 140639798581056 co_evolving_condition.py:127] Epoch :7, CL continuous loss: 1.000, discrete loss: 1.000\n",
      "I0902 21:55:11.221508 140639798581056 co_evolving_condition.py:128] Epoch :7, Total continuous loss: 1.272, discrete loss: 0.427\n",
      "I0902 21:55:12.547533 140639798581056 co_evolving_condition.py:126] Epoch :8, diffusion continuous loss: 0.983, discrete loss: 0.240\n",
      "I0902 21:55:12.548047 140639798581056 co_evolving_condition.py:127] Epoch :8, CL continuous loss: 1.000, discrete loss: 1.000\n",
      "I0902 21:55:12.548400 140639798581056 co_evolving_condition.py:128] Epoch :8, Total continuous loss: 1.183, discrete loss: 0.440\n",
      "I0902 21:55:13.939485 140639798581056 co_evolving_condition.py:126] Epoch :9, diffusion continuous loss: 1.018, discrete loss: 0.221\n",
      "I0902 21:55:13.940078 140639798581056 co_evolving_condition.py:127] Epoch :9, CL continuous loss: 1.000, discrete loss: 1.000\n",
      "I0902 21:55:13.940522 140639798581056 co_evolving_condition.py:128] Epoch :9, Total continuous loss: 1.218, discrete loss: 0.421\n",
      "I0902 21:55:15.158252 140639798581056 co_evolving_condition.py:126] Epoch :10, diffusion continuous loss: 0.952, discrete loss: 0.216\n",
      "I0902 21:55:15.161430 140639798581056 co_evolving_condition.py:127] Epoch :10, CL continuous loss: 1.000, discrete loss: 1.000\n",
      "I0902 21:55:15.161982 140639798581056 co_evolving_condition.py:128] Epoch :10, Total continuous loss: 1.152, discrete loss: 0.416\n",
      "I0902 21:55:16.385668 140639798581056 co_evolving_condition.py:126] Epoch :11, diffusion continuous loss: 1.011, discrete loss: 0.217\n",
      "I0902 21:55:16.386129 140639798581056 co_evolving_condition.py:127] Epoch :11, CL continuous loss: 1.000, discrete loss: 1.000\n",
      "I0902 21:55:16.386401 140639798581056 co_evolving_condition.py:128] Epoch :11, Total continuous loss: 1.211, discrete loss: 0.417\n",
      "I0902 21:55:17.637908 140639798581056 co_evolving_condition.py:126] Epoch :12, diffusion continuous loss: 1.065, discrete loss: 0.229\n",
      "I0902 21:55:17.638515 140639798581056 co_evolving_condition.py:127] Epoch :12, CL continuous loss: 1.000, discrete loss: 1.000\n",
      "I0902 21:55:17.638909 140639798581056 co_evolving_condition.py:128] Epoch :12, Total continuous loss: 1.265, discrete loss: 0.429\n",
      "I0902 21:55:18.902872 140639798581056 co_evolving_condition.py:126] Epoch :13, diffusion continuous loss: 0.949, discrete loss: 0.212\n",
      "I0902 21:55:18.903420 140639798581056 co_evolving_condition.py:127] Epoch :13, CL continuous loss: 0.999, discrete loss: 1.000\n",
      "I0902 21:55:18.903836 140639798581056 co_evolving_condition.py:128] Epoch :13, Total continuous loss: 1.148, discrete loss: 0.412\n",
      "I0902 21:55:20.225082 140639798581056 co_evolving_condition.py:126] Epoch :14, diffusion continuous loss: 1.002, discrete loss: 0.208\n",
      "I0902 21:55:20.225586 140639798581056 co_evolving_condition.py:127] Epoch :14, CL continuous loss: 1.000, discrete loss: 1.000\n",
      "I0902 21:55:20.225829 140639798581056 co_evolving_condition.py:128] Epoch :14, Total continuous loss: 1.202, discrete loss: 0.408\n",
      "I0902 21:55:21.493574 140639798581056 co_evolving_condition.py:126] Epoch :15, diffusion continuous loss: 0.941, discrete loss: 0.205\n",
      "I0902 21:55:21.494060 140639798581056 co_evolving_condition.py:127] Epoch :15, CL continuous loss: 1.001, discrete loss: 1.000\n",
      "I0902 21:55:21.494426 140639798581056 co_evolving_condition.py:128] Epoch :15, Total continuous loss: 1.142, discrete loss: 0.405\n",
      "I0902 21:55:22.704561 140639798581056 co_evolving_condition.py:126] Epoch :16, diffusion continuous loss: 0.908, discrete loss: 0.202\n",
      "I0902 21:55:22.706139 140639798581056 co_evolving_condition.py:127] Epoch :16, CL continuous loss: 0.997, discrete loss: 1.000\n",
      "I0902 21:55:22.706476 140639798581056 co_evolving_condition.py:128] Epoch :16, Total continuous loss: 1.108, discrete loss: 0.402\n",
      "I0902 21:55:23.338170 140639798581056 co_evolving_condition.py:126] Epoch :17, diffusion continuous loss: 0.973, discrete loss: 0.196\n",
      "I0902 21:55:23.339218 140639798581056 co_evolving_condition.py:127] Epoch :17, CL continuous loss: 1.001, discrete loss: 1.000\n",
      "I0902 21:55:23.339621 140639798581056 co_evolving_condition.py:128] Epoch :17, Total continuous loss: 1.173, discrete loss: 0.396\n",
      "I0902 21:55:24.581367 140639798581056 co_evolving_condition.py:126] Epoch :18, diffusion continuous loss: 1.009, discrete loss: 0.200\n",
      "I0902 21:55:24.581939 140639798581056 co_evolving_condition.py:127] Epoch :18, CL continuous loss: 1.002, discrete loss: 1.000\n",
      "I0902 21:55:24.582272 140639798581056 co_evolving_condition.py:128] Epoch :18, Total continuous loss: 1.210, discrete loss: 0.400\n",
      "I0902 21:55:25.771452 140639798581056 co_evolving_condition.py:126] Epoch :19, diffusion continuous loss: 0.826, discrete loss: 0.202\n",
      "I0902 21:55:25.772006 140639798581056 co_evolving_condition.py:127] Epoch :19, CL continuous loss: 1.000, discrete loss: 1.000\n",
      "I0902 21:55:25.772413 140639798581056 co_evolving_condition.py:128] Epoch :19, Total continuous loss: 1.026, discrete loss: 0.402\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/d/CoDi/main.py\", line 97, in <module>\n",
      "    app.run(main)\n",
      "  File \"/home/tomadonna/miniconda3/envs/codi/lib/python3.10/site-packages/absl/app.py\", line 308, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/home/tomadonna/miniconda3/envs/codi/lib/python3.10/site-packages/absl/app.py\", line 254, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"/mnt/d/CoDi/main.py\", line 94, in main\n",
      "    co_evolving_condition.train(FLAGS)\n",
      "  File \"/mnt/d/CoDi/co_evolving_condition.py\", line 177, in train\n",
      "    model_con.load_state_dict(ckpt['model_con'])\n",
      "  File \"/home/tomadonna/miniconda3/envs/codi/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1497, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for tabularUnet:\n",
      "\tsize mismatch for all_modules.2.weight: copying a param with shape torch.Size([2, 3]) from checkpoint, the shape in current model is torch.Size([2, 96]).\n",
      "\tsize mismatch for inputs.weight: copying a param with shape torch.Size([64, 7]) from checkpoint, the shape in current model is torch.Size([64, 4]).\n",
      "\tsize mismatch for outputs.weight: copying a param with shape torch.Size([5, 64]) from checkpoint, the shape in current model is torch.Size([2, 64]).\n",
      "\tsize mismatch for outputs.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Run using subprocess directly\n",
    "cmd = [\n",
    "    sys.executable, 'main.py',\n",
    "    '--data', 'Buddy',\n",
    "    '--total_epochs_both', '20',\n",
    "    '--training_batch_size', '1024',\n",
    "    '--num_samples', '500',\n",
    "    '--logdir', './CoDi_exp',\n",
    "    '--train'\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(\"STDOUT:\", result.stdout)\n",
    "print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f4a04b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tabular_datasets/iris.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_df\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Get clean combined dataset\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m combined_clean \u001b[38;5;241m=\u001b[39m \u001b[43mcombine_all_synthetic_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mClean combined dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m display(combined_clean\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m, in \u001b[0;36mcombine_all_synthetic_datasets\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./CoDi_exp/synthetic_data.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m     synthetic_datasets \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtabular_datasets/iris.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Get column names\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/codi/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tabular_datasets/iris.json'"
     ]
    }
   ],
   "source": [
    "# Enhanced version without dataset index\n",
    "def combine_all_synthetic_datasets():\n",
    "    \"\"\"Combine all synthetic datasets into one clean dataset (no dataset_idx column)\"\"\"\n",
    "    # Load data\n",
    "    with open('./CoDi_exp/synthetic_data.pkl', 'rb') as f:\n",
    "        synthetic_datasets = pickle.load(f)\n",
    "    \n",
    "    with open('tabular_datasets/iris.json', 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Get column names\n",
    "    column_names = [col['name'] for col in metadata['columns']]\n",
    "    \n",
    "    # Combine all raw data first (more efficient)\n",
    "    combined_raw_data = np.vstack(synthetic_datasets)\n",
    "    \n",
    "    # Create single DataFrame\n",
    "    combined_df = pd.DataFrame(combined_raw_data, columns=column_names)\n",
    "    \n",
    "    # Map categorical values\n",
    "    for col_info in metadata['columns']:\n",
    "        if col_info['type'] == 'categorical' and 'i2s' in col_info:\n",
    "            col_name = col_info['name']\n",
    "            i2s = col_info['i2s']\n",
    "            combined_df[col_name] = combined_df[col_name].round().astype(int).apply(\n",
    "                lambda x: i2s[x] if 0 <= x < len(i2s) else f\"unknown_{x}\"\n",
    "            )\n",
    "    \n",
    "    # print(f\"Combined {len(synthetic_datasets)} datasets\")\n",
    "    # print(f\"Total shape: {combined_df.shape}\")\n",
    "    # print(f\"Individual dataset shapes: {[data.shape for data in synthetic_datasets]}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "# Get clean combined dataset\n",
    "combined_clean = combine_all_synthetic_datasets()\n",
    "print(\"\\nClean combined dataset:\")\n",
    "display(combined_clean.head())\n",
    "display(combined_clean.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a99a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>75.500000</td>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>43.445368</td>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>38.250000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>75.500000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>112.750000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
       "count  150.000000     150.000000    150.000000     150.000000    150.000000\n",
       "mean    75.500000       5.843333      3.054000       3.758667      1.198667\n",
       "std     43.445368       0.828066      0.433594       1.764420      0.763161\n",
       "min      1.000000       4.300000      2.000000       1.000000      0.100000\n",
       "25%     38.250000       5.100000      2.800000       1.600000      0.300000\n",
       "50%     75.500000       5.800000      3.000000       4.350000      1.300000\n",
       "75%    112.750000       6.400000      3.300000       5.100000      1.800000\n",
       "max    150.000000       7.900000      4.400000       6.900000      2.500000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2ce505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f869b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
