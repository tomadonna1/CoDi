{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88142d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from typing import Tuple, List, Dict, Any, Optional\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15a3777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetProcessor:\n",
    "    \"\"\"Complete dataset processor for CoDi with automatic fixes and validation\"\"\"\n",
    "    \n",
    "    def __init__(self, categorical_threshold: int = 20, numeric_categorical_threshold: float = 0.05):\n",
    "        self.categorical_threshold = categorical_threshold\n",
    "        self.numeric_categorical_threshold = numeric_categorical_threshold\n",
    "    \n",
    "    def auto_detect_column_types(self, df: pd.DataFrame) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"Automatically detect continuous and categorical columns with improved logic\"\"\"\n",
    "        continuous_cols = []\n",
    "        categorical_cols = []\n",
    "        \n",
    "        print(\"Analyzing column types...\")\n",
    "        for col in df.columns:\n",
    "            col_data = df[col].dropna()  # Remove NaN for analysis\n",
    "            unique_count = col_data.nunique()\n",
    "            total_count = len(col_data)\n",
    "            unique_ratio = unique_count / total_count if total_count > 0 else 0\n",
    "            \n",
    "            # Check if column contains only integers (potential categorical)\n",
    "            is_integer_like = False\n",
    "            if pd.api.types.is_numeric_dtype(col_data):\n",
    "                is_integer_like = col_data.apply(lambda x: float(x).is_integer()).all()\n",
    "            \n",
    "            # Enhanced decision logic\n",
    "            if pd.api.types.is_numeric_dtype(col_data):\n",
    "                # Special case: floating point values that are actually discrete\n",
    "                if is_integer_like and (unique_count <= self.categorical_threshold or unique_ratio < self.numeric_categorical_threshold):\n",
    "                    categorical_cols.append(col)\n",
    "                    print(f\"'{col}': Numeric categorical ({unique_count} unique, ratio: {unique_ratio:.3f})\")\n",
    "                # Special case: many decimal values suggest continuous\n",
    "                elif not is_integer_like and unique_count > self.categorical_threshold:\n",
    "                    continuous_cols.append(col)\n",
    "                    print(f\"'{col}': Continuous decimal ({unique_count} unique)\")\n",
    "                # Default numeric logic\n",
    "                elif unique_count <= self.categorical_threshold or unique_ratio < self.numeric_categorical_threshold:\n",
    "                    categorical_cols.append(col)\n",
    "                    print(f\"'{col}': Numeric categorical ({unique_count} unique, ratio: {unique_ratio:.3f})\")\n",
    "                else:\n",
    "                    continuous_cols.append(col)\n",
    "                    print(f\"'{col}': Continuous ({unique_count} unique)\")\n",
    "            else:\n",
    "                # Non-numeric -> categorical\n",
    "                categorical_cols.append(col)\n",
    "                print(f\"'{col}': Text categorical ({unique_count} unique)\")\n",
    "        \n",
    "        return continuous_cols, categorical_cols\n",
    "    \n",
    "    def preprocess_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"Enhanced preprocessing with better missing value handling\"\"\"\n",
    "        df_processed = df.copy()\n",
    "        categorical_mappings = {}\n",
    "        \n",
    "        print(\"\\nPreprocessing data...\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        for col in df_processed.columns:\n",
    "            if df_processed[col].isnull().any():\n",
    "                null_count = df_processed[col].isnull().sum()\n",
    "                if pd.api.types.is_numeric_dtype(df_processed[col]):\n",
    "                    df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "                    print(f\"Filled {null_count} missing values in '{col}' with median\")\n",
    "                else:\n",
    "                    mode_val = df_processed[col].mode()\n",
    "                    if len(mode_val) > 0:\n",
    "                        df_processed[col].fillna(mode_val[0], inplace=True)\n",
    "                    else:\n",
    "                        df_processed[col].fillna('unknown', inplace=True)\n",
    "                    print(f\"Filled {null_count} missing values in '{col}' with mode/unknown\")\n",
    "        \n",
    "        # Encode categorical variables with proper indexing\n",
    "        for col in df_processed.columns:\n",
    "            if not pd.api.types.is_numeric_dtype(df_processed[col]):\n",
    "                unique_vals = sorted(df_processed[col].unique())\n",
    "                mapping = {val: idx for idx, val in enumerate(unique_vals)}\n",
    "                categorical_mappings[col] = {\n",
    "                    'mapping': mapping,\n",
    "                    'reverse_mapping': {idx: val for val, idx in mapping.items()}\n",
    "                }\n",
    "                df_processed[col] = df_processed[col].map(mapping)\n",
    "                print(f\"Encoded '{col}': {len(unique_vals)} categories -> [0, {len(unique_vals)-1}]\")\n",
    "        \n",
    "        return df_processed, categorical_mappings\n",
    "    \n",
    "    def validate_and_fix_categorical_data(self, data: np.ndarray, columns: List[Dict]) -> Tuple[np.ndarray, List[Dict]]:\n",
    "        \"\"\"Validate and fix categorical columns to ensure proper 0-based indexing\"\"\"\n",
    "        print(\"\\nValidating and fixing categorical data...\")\n",
    "        fixed_data = data.copy()\n",
    "        fixed_columns = [col.copy() for col in columns]\n",
    "        \n",
    "        for i, col in enumerate(fixed_columns):\n",
    "            if col['type'] == 'categorical':\n",
    "                col_data = fixed_data[:, i].astype(int)\n",
    "                unique_vals = sorted(np.unique(col_data))\n",
    "                \n",
    "                # Check if values are properly 0-based\n",
    "                expected_range = list(range(len(unique_vals)))\n",
    "                if unique_vals != expected_range:\n",
    "                    print(f\"Fixing '{col['name']}': {unique_vals} -> {expected_range}\")\n",
    "                    \n",
    "                    # Create mapping to fix indexing\n",
    "                    mapping = {old_val: new_val for new_val, old_val in enumerate(unique_vals)}\n",
    "                    \n",
    "                    # Apply mapping\n",
    "                    for old_val, new_val in mapping.items():\n",
    "                        fixed_data[fixed_data[:, i] == old_val, i] = new_val\n",
    "                    \n",
    "                    # Update column metadata\n",
    "                    col['size'] = len(unique_vals)\n",
    "                    col['i2s'] = [str(val) for val in unique_vals]\n",
    "                else:\n",
    "                    print(f\"'{col['name']}': Already properly indexed [0, {len(unique_vals)-1}]\")\n",
    "        \n",
    "        return fixed_data, fixed_columns\n",
    "    \n",
    "    def create_codi_format(self, dataset_name: str, train_data: np.ndarray, test_data: np.ndarray, \n",
    "                          column_names: List[str], con_idx: List[int], dis_idx: List[int], \n",
    "                          categorical_mappings: Dict) -> Dict:\n",
    "        \"\"\"Create CoDi-compatible format with validation\"\"\"\n",
    "        \n",
    "        # Validate and fix categorical data\n",
    "        all_data = np.vstack([train_data, test_data])\n",
    "        \n",
    "        # Create initial columns structure\n",
    "        columns = []\n",
    "        for i, col_name in enumerate(column_names):\n",
    "            if i in con_idx:\n",
    "                col_data = all_data[:, i]\n",
    "                columns.append({\n",
    "                    \"name\": col_name,\n",
    "                    \"type\": \"continuous\",\n",
    "                    \"min\": float(np.min(col_data)),\n",
    "                    \"max\": float(np.max(col_data))\n",
    "                })\n",
    "            else:\n",
    "                col_data = all_data[:, i].astype(int)\n",
    "                unique_vals = sorted(np.unique(col_data))\n",
    "                \n",
    "                # Create i2s mapping\n",
    "                if col_name in categorical_mappings:\n",
    "                    reverse_mapping = categorical_mappings[col_name]['reverse_mapping']\n",
    "                    i2s = [str(reverse_mapping.get(idx, str(idx))) for idx in unique_vals]\n",
    "                else:\n",
    "                    i2s = [str(val) for val in unique_vals]\n",
    "                \n",
    "                columns.append({\n",
    "                    \"name\": col_name,\n",
    "                    \"type\": \"categorical\",\n",
    "                    \"size\": len(unique_vals),\n",
    "                    \"i2s\": i2s\n",
    "                })\n",
    "        \n",
    "        # Fix categorical data indexing\n",
    "        fixed_train, fixed_columns = self.validate_and_fix_categorical_data(train_data, columns)\n",
    "        fixed_test, _ = self.validate_and_fix_categorical_data(test_data, columns)\n",
    "        \n",
    "        # Determine problem type\n",
    "        last_col_idx = len(column_names) - 1\n",
    "        if last_col_idx in dis_idx:\n",
    "            last_col_name = column_names[last_col_idx]\n",
    "            if last_col_name in categorical_mappings:\n",
    "                num_classes = len(categorical_mappings[last_col_name]['mapping'])\n",
    "            else:\n",
    "                num_classes = len(np.unique(all_data[:, last_col_idx]))\n",
    "            \n",
    "            problem_type = \"binary_classification\" if num_classes == 2 else \"multiclass_classification\"\n",
    "        else:\n",
    "            problem_type = \"regression\"\n",
    "        \n",
    "        # Save fixed data\n",
    "        os.makedirs('tabular_datasets', exist_ok=True)\n",
    "        np.savez(f'tabular_datasets/{dataset_name}.npz', train=fixed_train, test=fixed_test)\n",
    "        \n",
    "        # Create metadata\n",
    "        codi_meta = {\n",
    "            \"columns\": fixed_columns,\n",
    "            \"problem_type\": problem_type\n",
    "        }\n",
    "        \n",
    "        with open(f'tabular_datasets/{dataset_name}.json', 'w') as f:\n",
    "            json.dump(codi_meta, f, indent=2)\n",
    "        \n",
    "        return codi_meta\n",
    "    \n",
    "    def process_dataset(self, csv_path: str, dataset_name: str, \n",
    "                       force_continuous: Optional[List[str]] = None,\n",
    "                       force_categorical: Optional[List[str]] = None,\n",
    "                       test_split: float = 0.2,\n",
    "                       create_backup: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Complete dataset processing pipeline\"\"\"\n",
    "        \n",
    "        force_continuous = force_continuous or []\n",
    "        force_categorical = force_categorical or []\n",
    "        \n",
    "        print(f\"Processing dataset: {csv_path} -> {dataset_name}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Create backup if requested\n",
    "        if create_backup and os.path.exists(f'tabular_datasets/{dataset_name}.npz'):\n",
    "            print(\"Creating backup of existing dataset...\")\n",
    "            shutil.copy(f'tabular_datasets/{dataset_name}.npz', f'tabular_datasets/{dataset_name}_backup.npz')\n",
    "            if os.path.exists(f'tabular_datasets/{dataset_name}.json'):\n",
    "                shutil.copy(f'tabular_datasets/{dataset_name}.json', f'tabular_datasets/{dataset_name}_backup.json')\n",
    "        \n",
    "        # Load and analyze data\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Original shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Auto-detect column types\n",
    "        continuous_cols, categorical_cols = self.auto_detect_column_types(df)\n",
    "        \n",
    "        # Apply manual overrides\n",
    "        if force_continuous or force_categorical:\n",
    "            print(f\"\\nApplying manual overrides...\")\n",
    "            for col in force_continuous:\n",
    "                if col in categorical_cols:\n",
    "                    categorical_cols.remove(col)\n",
    "                if col not in continuous_cols:\n",
    "                    continuous_cols.append(col)\n",
    "                print(f\"Forced '{col}' to continuous\")\n",
    "            \n",
    "            for col in force_categorical:\n",
    "                if col in continuous_cols:\n",
    "                    continuous_cols.remove(col)\n",
    "                if col not in categorical_cols:\n",
    "                    categorical_cols.append(col)\n",
    "                print(f\"Forced '{col}' to categorical\")\n",
    "        \n",
    "        print(f\"\\nFinal column assignment:\")\n",
    "        print(f\"Continuous ({len(continuous_cols)}): {continuous_cols}\")\n",
    "        print(f\"Categorical ({len(categorical_cols)}): {categorical_cols}\")\n",
    "        \n",
    "        # Preprocess data\n",
    "        df_processed, categorical_mappings = self.preprocess_data(df)\n",
    "        \n",
    "        # Get indices\n",
    "        con_idx = [df_processed.columns.get_loc(col) for col in continuous_cols]\n",
    "        dis_idx = [df_processed.columns.get_loc(col) for col in categorical_cols]\n",
    "        \n",
    "        # Split data\n",
    "        data = df_processed.values.astype(np.float32)\n",
    "        n_samples, n_features = data.shape\n",
    "        n_test = int(n_samples * test_split)\n",
    "        \n",
    "        # Shuffle and split\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        test_data = data[indices[:n_test]]\n",
    "        train_data = data[indices[n_test:]]\n",
    "        \n",
    "        print(f\"\\nData split:\")\n",
    "        print(f\"Training samples: {len(train_data)}\")\n",
    "        print(f\"Test samples: {len(test_data)}\")\n",
    "        \n",
    "        # Create CoDi format with validation and fixes\n",
    "        codi_meta = self.create_codi_format(\n",
    "            dataset_name, train_data, test_data, \n",
    "            df_processed.columns.tolist(), con_idx, dis_idx, categorical_mappings\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nDataset '{dataset_name}' processed successfully!\")\n",
    "        print(f\"Problem type: {codi_meta['problem_type']}\")\n",
    "        print(f\"Files created:\")\n",
    "        print(f\"  - tabular_datasets/{dataset_name}.npz\")\n",
    "        print(f\"  - tabular_datasets/{dataset_name}.json\")\n",
    "        \n",
    "        return {\n",
    "            'dataset_name': dataset_name,\n",
    "            'shape': (n_samples, n_features),\n",
    "            'problem_type': codi_meta['problem_type'],\n",
    "            'continuous_columns': continuous_cols,\n",
    "            'categorical_columns': categorical_cols,\n",
    "            'train_samples': len(train_data),\n",
    "            'test_samples': len(test_data)\n",
    "        }\n",
    "    \n",
    "    def validate_dataset(self, dataset_name: str) -> bool:\n",
    "        \"\"\"Validate that a dataset is properly formatted for CoDi\"\"\"\n",
    "        try:\n",
    "            data = np.load(f'tabular_datasets/{dataset_name}.npz')\n",
    "            with open(f'tabular_datasets/{dataset_name}.json', 'r') as f:\n",
    "                meta = json.load(f)\n",
    "            \n",
    "            train_data = data['train']\n",
    "            print(f\"Validating dataset: {dataset_name}\")\n",
    "            print(\"=\"*40)\n",
    "            \n",
    "            issues_found = False\n",
    "            \n",
    "            for i, col in enumerate(meta['columns']):\n",
    "                if col['type'] == 'categorical':\n",
    "                    col_data = train_data[:, i].astype(int)\n",
    "                    unique_vals = np.unique(col_data)\n",
    "                    min_val, max_val = np.min(col_data), np.max(col_data)\n",
    "                    \n",
    "                    if min_val < 0 or max_val >= col['size']:\n",
    "                        print(f\"{col['name']}: values [{min_val}, {max_val}] outside expected [0, {col['size']-1}]\")\n",
    "                        issues_found = True\n",
    "                    else:\n",
    "                        print(f\"{col['name']}: properly indexed [0, {col['size']-1}]\")\n",
    "                else:\n",
    "                    print(f\"{col['name']}: continuous column OK\")\n",
    "            \n",
    "            if not issues_found:\n",
    "                print(f\"\\nDataset '{dataset_name}' is valid for CoDi training!\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"\\nDataset '{dataset_name}' has validation issues.\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Validation failed: {e}\")\n",
    "            return False\n",
    "\n",
    "# Usage functions\n",
    "def quick_process(csv_path: str, dataset_name: str, **kwargs) -> Dict[str, Any]:\n",
    "    \"\"\"Quick processing with default settings\"\"\"\n",
    "    processor = DatasetProcessor()\n",
    "    return processor.process_dataset(csv_path, dataset_name, **kwargs)\n",
    "\n",
    "def process_with_overrides(csv_path: str, dataset_name: str, \n",
    "                          continuous_cols: List[str] = None,\n",
    "                          categorical_cols: List[str] = None, **kwargs) -> Dict[str, Any]:\n",
    "    \"\"\"Process with manual column type specification\"\"\"\n",
    "    processor = DatasetProcessor()\n",
    "    return processor.process_dataset(\n",
    "        csv_path, dataset_name, \n",
    "        force_continuous=continuous_cols,\n",
    "        force_categorical=categorical_cols,\n",
    "        **kwargs\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b6c1e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id               0\n",
       "SepalLengthCm    0\n",
       "SepalWidthCm     0\n",
       "PetalLengthCm    0\n",
       "PetalWidthCm     0\n",
       "Species          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_original = pd.read_csv(\"raw_data/iris.csv\")\n",
    "iris_original.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe87ce03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: raw_data/iris.csv -> iris\n",
      "============================================================\n",
      "Creating backup of existing dataset...\n",
      "Original shape: (150, 6)\n",
      "Columns: ['Id', 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species']\n",
      "Analyzing column types...\n",
      "'Id': Continuous (150 unique)\n",
      "'SepalLengthCm': Continuous decimal (35 unique)\n",
      "'SepalWidthCm': Continuous decimal (23 unique)\n",
      "'PetalLengthCm': Continuous decimal (43 unique)\n",
      "'PetalWidthCm': Continuous decimal (22 unique)\n",
      "'Species': Text categorical (3 unique)\n",
      "\n",
      "Final column assignment:\n",
      "Continuous (5): ['Id', 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
      "Categorical (1): ['Species']\n",
      "\n",
      "Preprocessing data...\n",
      "Encoded 'Species': 3 categories -> [0, 2]\n",
      "\n",
      "Data split:\n",
      "Training samples: 120\n",
      "Test samples: 30\n",
      "\n",
      "Validating and fixing categorical data...\n",
      "'Species': Already properly indexed [0, 2]\n",
      "\n",
      "Validating and fixing categorical data...\n",
      "'Species': Already properly indexed [0, 2]\n",
      "\n",
      "Dataset 'iris' processed successfully!\n",
      "Problem type: multiclass_classification\n",
      "Files created:\n",
      "  - tabular_datasets/iris.npz\n",
      "  - tabular_datasets/iris.json\n",
      "Validating dataset: iris\n",
      "========================================\n",
      "Id: continuous column OK\n",
      "SepalLengthCm: continuous column OK\n",
      "SepalWidthCm: continuous column OK\n",
      "PetalLengthCm: continuous column OK\n",
      "PetalWidthCm: continuous column OK\n",
      "Species: properly indexed [0, 2]\n",
      "\n",
      "Dataset 'iris' is valid for CoDi training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "processor = DatasetProcessor(categorical_threshold=15, numeric_categorical_threshold=0.05)\n",
    "\n",
    "# Process a dataset\n",
    "result = processor.process_dataset(\n",
    "    csv_path='raw_data/iris.csv',\n",
    "    dataset_name='iris',\n",
    "    test_split=0.2,\n",
    "    create_backup=True\n",
    ")\n",
    "\n",
    "# Validate the result\n",
    "processor.validate_dataset('iris')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532ad66",
   "metadata": {},
   "source": [
    "python main.py --data iris --logdir CoDi_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8cfa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: I0902 20:54:20.012278 140093344847680 main.py:93] Co-evolving Conditional Diffusion models\n",
      "I0902 20:54:23.486226 140093344847680 co_evolving_condition.py:69] Continuous model params: 450829\n",
      "I0902 20:54:23.486808 140093344847680 co_evolving_condition.py:70] Discrete model params: 675285\n",
      "I0902 20:54:23.487200 140093344847680 co_evolving_condition.py:76] Total steps: 20\n",
      "I0902 20:54:23.487432 140093344847680 co_evolving_condition.py:77] Sample steps: 2000\n",
      "I0902 20:54:23.487679 140093344847680 co_evolving_condition.py:78] Continuous: 120, 5\n",
      "I0902 20:54:23.487900 140093344847680 co_evolving_condition.py:79] Discrete: 120, 3\n",
      "I0902 20:54:24.515496 140093344847680 co_evolving_condition.py:126] Epoch :0, diffusion continuous loss: 0.924, discrete loss: 0.325\n",
      "I0902 20:54:24.516203 140093344847680 co_evolving_condition.py:127] Epoch :0, CL continuous loss: 0.993, discrete loss: 1.000\n",
      "I0902 20:54:24.516596 140093344847680 co_evolving_condition.py:128] Epoch :0, Total continuous loss: 1.123, discrete loss: 0.525\n",
      "I0902 20:54:24.574172 140093344847680 co_evolving_condition.py:126] Epoch :1, diffusion continuous loss: 0.960, discrete loss: 0.463\n",
      "I0902 20:54:24.574755 140093344847680 co_evolving_condition.py:127] Epoch :1, CL continuous loss: 1.000, discrete loss: 1.000\n",
      "I0902 20:54:24.575056 140093344847680 co_evolving_condition.py:128] Epoch :1, Total continuous loss: 1.160, discrete loss: 0.663\n",
      "I0902 20:54:24.628222 140093344847680 co_evolving_condition.py:126] Epoch :2, diffusion continuous loss: 1.102, discrete loss: 0.359\n",
      "I0902 20:54:24.629065 140093344847680 co_evolving_condition.py:127] Epoch :2, CL continuous loss: 1.003, discrete loss: 1.000\n",
      "I0902 20:54:24.629538 140093344847680 co_evolving_condition.py:128] Epoch :2, Total continuous loss: 1.303, discrete loss: 0.559\n",
      "I0902 20:54:24.671360 140093344847680 co_evolving_condition.py:126] Epoch :3, diffusion continuous loss: 1.014, discrete loss: 0.343\n",
      "I0902 20:54:24.671870 140093344847680 co_evolving_condition.py:127] Epoch :3, CL continuous loss: 1.003, discrete loss: 1.000\n",
      "I0902 20:54:24.672272 140093344847680 co_evolving_condition.py:128] Epoch :3, Total continuous loss: 1.215, discrete loss: 0.544\n",
      "I0902 20:54:24.739046 140093344847680 co_evolving_condition.py:126] Epoch :4, diffusion continuous loss: 1.057, discrete loss: 0.444\n",
      "I0902 20:54:24.739601 140093344847680 co_evolving_condition.py:127] Epoch :4, CL continuous loss: 1.008, discrete loss: 1.000\n",
      "I0902 20:54:24.739895 140093344847680 co_evolving_condition.py:128] Epoch :4, Total continuous loss: 1.259, discrete loss: 0.644\n",
      "I0902 20:54:24.785325 140093344847680 co_evolving_condition.py:126] Epoch :5, diffusion continuous loss: 0.987, discrete loss: 0.247\n",
      "I0902 20:54:24.786060 140093344847680 co_evolving_condition.py:127] Epoch :5, CL continuous loss: 0.997, discrete loss: 1.000\n",
      "I0902 20:54:24.786408 140093344847680 co_evolving_condition.py:128] Epoch :5, Total continuous loss: 1.186, discrete loss: 0.447\n",
      "I0902 20:54:24.840922 140093344847680 co_evolving_condition.py:126] Epoch :6, diffusion continuous loss: 0.959, discrete loss: 0.376\n",
      "I0902 20:54:24.841511 140093344847680 co_evolving_condition.py:127] Epoch :6, CL continuous loss: 0.997, discrete loss: 1.000\n",
      "I0902 20:54:24.841804 140093344847680 co_evolving_condition.py:128] Epoch :6, Total continuous loss: 1.159, discrete loss: 0.576\n",
      "I0902 20:54:24.905385 140093344847680 co_evolving_condition.py:126] Epoch :7, diffusion continuous loss: 1.063, discrete loss: 0.331\n",
      "I0902 20:54:24.905968 140093344847680 co_evolving_condition.py:127] Epoch :7, CL continuous loss: 1.002, discrete loss: 1.000\n",
      "I0902 20:54:24.906286 140093344847680 co_evolving_condition.py:128] Epoch :7, Total continuous loss: 1.264, discrete loss: 0.531\n",
      "I0902 20:54:24.955566 140093344847680 co_evolving_condition.py:126] Epoch :8, diffusion continuous loss: 1.039, discrete loss: 0.384\n",
      "I0902 20:54:24.956161 140093344847680 co_evolving_condition.py:127] Epoch :8, CL continuous loss: 0.993, discrete loss: 1.000\n",
      "I0902 20:54:24.956596 140093344847680 co_evolving_condition.py:128] Epoch :8, Total continuous loss: 1.237, discrete loss: 0.584\n",
      "I0902 20:54:25.008306 140093344847680 co_evolving_condition.py:126] Epoch :9, diffusion continuous loss: 1.033, discrete loss: 0.448\n",
      "I0902 20:54:25.008982 140093344847680 co_evolving_condition.py:127] Epoch :9, CL continuous loss: 1.000, discrete loss: 1.000\n",
      "I0902 20:54:25.009344 140093344847680 co_evolving_condition.py:128] Epoch :9, Total continuous loss: 1.233, discrete loss: 0.648\n",
      "I0902 20:54:25.057164 140093344847680 co_evolving_condition.py:126] Epoch :10, diffusion continuous loss: 1.004, discrete loss: 0.431\n",
      "I0902 20:54:25.057766 140093344847680 co_evolving_condition.py:127] Epoch :10, CL continuous loss: 1.008, discrete loss: 1.000\n",
      "I0902 20:54:25.058100 140093344847680 co_evolving_condition.py:128] Epoch :10, Total continuous loss: 1.205, discrete loss: 0.631\n",
      "I0902 20:54:25.102286 140093344847680 co_evolving_condition.py:126] Epoch :11, diffusion continuous loss: 0.971, discrete loss: 0.278\n",
      "I0902 20:54:25.102957 140093344847680 co_evolving_condition.py:127] Epoch :11, CL continuous loss: 0.999, discrete loss: 1.000\n",
      "I0902 20:54:25.103324 140093344847680 co_evolving_condition.py:128] Epoch :11, Total continuous loss: 1.170, discrete loss: 0.478\n",
      "I0902 20:54:25.158344 140093344847680 co_evolving_condition.py:126] Epoch :12, diffusion continuous loss: 0.961, discrete loss: 0.316\n",
      "I0902 20:54:25.158956 140093344847680 co_evolving_condition.py:127] Epoch :12, CL continuous loss: 1.002, discrete loss: 1.000\n",
      "I0902 20:54:25.159358 140093344847680 co_evolving_condition.py:128] Epoch :12, Total continuous loss: 1.161, discrete loss: 0.516\n",
      "I0902 20:54:25.208433 140093344847680 co_evolving_condition.py:126] Epoch :13, diffusion continuous loss: 1.056, discrete loss: 0.350\n",
      "I0902 20:54:25.209133 140093344847680 co_evolving_condition.py:127] Epoch :13, CL continuous loss: 0.998, discrete loss: 1.000\n",
      "I0902 20:54:25.209547 140093344847680 co_evolving_condition.py:128] Epoch :13, Total continuous loss: 1.256, discrete loss: 0.550\n",
      "I0902 20:54:25.261632 140093344847680 co_evolving_condition.py:126] Epoch :14, diffusion continuous loss: 0.955, discrete loss: 0.309\n",
      "I0902 20:54:25.262250 140093344847680 co_evolving_condition.py:127] Epoch :14, CL continuous loss: 0.998, discrete loss: 1.000\n",
      "I0902 20:54:25.262618 140093344847680 co_evolving_condition.py:128] Epoch :14, Total continuous loss: 1.154, discrete loss: 0.509\n",
      "I0902 20:54:25.304818 140093344847680 co_evolving_condition.py:126] Epoch :15, diffusion continuous loss: 1.026, discrete loss: 0.321\n",
      "I0902 20:54:25.305353 140093344847680 co_evolving_condition.py:127] Epoch :15, CL continuous loss: 0.999, discrete loss: 1.000\n",
      "I0902 20:54:25.305746 140093344847680 co_evolving_condition.py:128] Epoch :15, Total continuous loss: 1.225, discrete loss: 0.521\n",
      "I0902 20:54:25.351682 140093344847680 co_evolving_condition.py:126] Epoch :16, diffusion continuous loss: 1.105, discrete loss: 0.340\n",
      "I0902 20:54:25.352659 140093344847680 co_evolving_condition.py:127] Epoch :16, CL continuous loss: 1.002, discrete loss: 1.000\n",
      "I0902 20:54:25.353250 140093344847680 co_evolving_condition.py:128] Epoch :16, Total continuous loss: 1.305, discrete loss: 0.540\n",
      "I0902 20:54:25.397914 140093344847680 co_evolving_condition.py:126] Epoch :17, diffusion continuous loss: 0.940, discrete loss: 0.380\n",
      "I0902 20:54:25.398459 140093344847680 co_evolving_condition.py:127] Epoch :17, CL continuous loss: 0.999, discrete loss: 1.000\n",
      "I0902 20:54:25.398792 140093344847680 co_evolving_condition.py:128] Epoch :17, Total continuous loss: 1.140, discrete loss: 0.580\n",
      "I0902 20:54:25.442608 140093344847680 co_evolving_condition.py:126] Epoch :18, diffusion continuous loss: 0.960, discrete loss: 0.292\n",
      "I0902 20:54:25.443203 140093344847680 co_evolving_condition.py:127] Epoch :18, CL continuous loss: 1.000, discrete loss: 1.000\n",
      "I0902 20:54:25.443536 140093344847680 co_evolving_condition.py:128] Epoch :18, Total continuous loss: 1.160, discrete loss: 0.492\n",
      "I0902 20:54:25.498789 140093344847680 co_evolving_condition.py:126] Epoch :19, diffusion continuous loss: 1.018, discrete loss: 0.318\n",
      "I0902 20:54:25.499445 140093344847680 co_evolving_condition.py:127] Epoch :19, CL continuous loss: 0.996, discrete loss: 1.000\n",
      "I0902 20:54:25.499714 140093344847680 co_evolving_condition.py:128] Epoch :19, Total continuous loss: 1.217, discrete loss: 0.518\n",
      "I0902 20:54:25.661598 140093344847680 co_evolving_condition.py:201] sampling 500 samples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Run using subprocess directly\n",
    "cmd = [\n",
    "    sys.executable, 'main.py',\n",
    "    '--data', 'iris',\n",
    "    '--total_epochs_both', '20',\n",
    "    '--training_batch_size', '1024',\n",
    "    '--num_samples', '500',\n",
    "    '--logdir', './CoDi_exp',\n",
    "    '--train'\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(\"STDOUT:\", result.stdout)\n",
    "print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a04b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 1 datasets\n",
      "Total shape: (500, 6)\n",
      "Individual dataset shapes: [(500, 6)]\n",
      "\n",
      "Clean combined dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59.968529</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>3.014778</td>\n",
       "      <td>6.516171</td>\n",
       "      <td>0.615078</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.079647</td>\n",
       "      <td>7.462254</td>\n",
       "      <td>3.787957</td>\n",
       "      <td>3.519754</td>\n",
       "      <td>1.371104</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125.315575</td>\n",
       "      <td>7.409658</td>\n",
       "      <td>4.088545</td>\n",
       "      <td>2.945665</td>\n",
       "      <td>0.434851</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79.446953</td>\n",
       "      <td>5.459857</td>\n",
       "      <td>3.185479</td>\n",
       "      <td>6.396809</td>\n",
       "      <td>2.073986</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.423725</td>\n",
       "      <td>6.788310</td>\n",
       "      <td>2.891448</td>\n",
       "      <td>4.659242</td>\n",
       "      <td>1.856624</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n",
       "0   59.968529       7.700000      3.014778       6.516171      0.615078   \n",
       "1   40.079647       7.462254      3.787957       3.519754      1.371104   \n",
       "2  125.315575       7.409658      4.088545       2.945665      0.434851   \n",
       "3   79.446953       5.459857      3.185479       6.396809      2.073986   \n",
       "4   24.423725       6.788310      2.891448       4.659242      1.856624   \n",
       "\n",
       "           Species  \n",
       "0   Iris-virginica  \n",
       "1  Iris-versicolor  \n",
       "2      Iris-setosa  \n",
       "3   Iris-virginica  \n",
       "4      Iris-setosa  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>77.346307</td>\n",
       "      <td>6.272344</td>\n",
       "      <td>3.235651</td>\n",
       "      <td>4.446397</td>\n",
       "      <td>1.356670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>48.727254</td>\n",
       "      <td>1.130016</td>\n",
       "      <td>0.775684</td>\n",
       "      <td>1.873476</td>\n",
       "      <td>0.789619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.300436</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.002051</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.615707</td>\n",
       "      <td>5.242195</td>\n",
       "      <td>2.523090</td>\n",
       "      <td>2.744202</td>\n",
       "      <td>0.613988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>76.127281</td>\n",
       "      <td>6.425347</td>\n",
       "      <td>3.228642</td>\n",
       "      <td>4.854560</td>\n",
       "      <td>1.348977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>120.902212</td>\n",
       "      <td>7.407302</td>\n",
       "      <td>4.001225</td>\n",
       "      <td>6.245686</td>\n",
       "      <td>2.097555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.700000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
       "count  500.000000     500.000000    500.000000     500.000000    500.000000\n",
       "mean    77.346307       6.272344      3.235651       4.446397      1.356670\n",
       "std     48.727254       1.130016      0.775684       1.873476      0.789619\n",
       "min      1.000000       4.300436      2.000000       1.002051      0.100000\n",
       "25%     32.615707       5.242195      2.523090       2.744202      0.613988\n",
       "50%     76.127281       6.425347      3.228642       4.854560      1.348977\n",
       "75%    120.902212       7.407302      4.001225       6.245686      2.097555\n",
       "max    150.000000       7.700000      4.400000       6.700000      2.500000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enhanced version without dataset index\n",
    "def combine_all_synthetic_datasets():\n",
    "    \"\"\"Combine all synthetic datasets into one clean dataset (no dataset_idx column)\"\"\"\n",
    "    # Load data\n",
    "    with open('./CoDi_exp/synthetic_data.pkl', 'rb') as f:\n",
    "        synthetic_datasets = pickle.load(f)\n",
    "    \n",
    "    with open('tabular_datasets/iris.json', 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Get column names\n",
    "    column_names = [col['name'] for col in metadata['columns']]\n",
    "    \n",
    "    # Combine all raw data first (more efficient)\n",
    "    combined_raw_data = np.vstack(synthetic_datasets)\n",
    "    \n",
    "    # Create single DataFrame\n",
    "    combined_df = pd.DataFrame(combined_raw_data, columns=column_names)\n",
    "    \n",
    "    # Map categorical values\n",
    "    for col_info in metadata['columns']:\n",
    "        if col_info['type'] == 'categorical' and 'i2s' in col_info:\n",
    "            col_name = col_info['name']\n",
    "            i2s = col_info['i2s']\n",
    "            combined_df[col_name] = combined_df[col_name].round().astype(int).apply(\n",
    "                lambda x: i2s[x] if 0 <= x < len(i2s) else f\"unknown_{x}\"\n",
    "            )\n",
    "    \n",
    "    # print(f\"Combined {len(synthetic_datasets)} datasets\")\n",
    "    # print(f\"Total shape: {combined_df.shape}\")\n",
    "    # print(f\"Individual dataset shapes: {[data.shape for data in synthetic_datasets]}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "# Get clean combined dataset\n",
    "combined_clean = combine_all_synthetic_datasets()\n",
    "print(\"\\nClean combined dataset:\")\n",
    "display(combined_clean.head())\n",
    "display(combined_clean.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea0a99a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>75.500000</td>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>43.445368</td>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>38.250000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>75.500000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>112.750000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
       "count  150.000000     150.000000    150.000000     150.000000    150.000000\n",
       "mean    75.500000       5.843333      3.054000       3.758667      1.198667\n",
       "std     43.445368       0.828066      0.433594       1.764420      0.763161\n",
       "min      1.000000       4.300000      2.000000       1.000000      0.100000\n",
       "25%     38.250000       5.100000      2.800000       1.600000      0.300000\n",
       "50%     75.500000       5.800000      3.000000       4.350000      1.300000\n",
       "75%    112.750000       6.400000      3.300000       5.100000      1.800000\n",
       "max    150.000000       7.900000      4.400000       6.900000      2.500000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae2ce505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f869b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
